{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwwHkrhx-1JS"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install streamlit PyPDF2 langchain-google-genai faiss-cpu langchain_community\n",
        "!pip install ngrok\n",
        "!pip install -q pyngrok\n",
        "\n",
        "\n",
        "# Set your Google API Key (replace with your actual key)\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"google api key\"\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = \"ngrok auth token\"#--- IMPORTANT: REPLACE THIS!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JtC8MmC-8ol"
      },
      "outputs": [],
      "source": [
        "\n",
        "### `app.py` Code\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import PyPDF2\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_huggingface import HuggingFaceEmbeddings  # Add this import\n",
        "import asyncio\n",
        "\n",
        "# Helper Functions\n",
        "def get_pdf_text(pdf_docs):\n",
        "    \"\"\"Extracts text from uploaded PDF files.\"\"\"\n",
        "    text = \"\"\n",
        "    for pdf in pdf_docs:\n",
        "        try:\n",
        "            pdf_reader = PyPDF2.PdfReader(pdf)\n",
        "            for page in pdf_reader.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += page_text\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error reading file {pdf.name}: {e}\")\n",
        "            raise\n",
        "    return text\n",
        "\n",
        "def get_text_chunks(text):\n",
        "    \"\"\"Splits text into manageable chunks.\"\"\"\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "    return splitter.split_text(text)\n",
        "\n",
        "def get_vector_store(text_chunks):\n",
        "    \"\"\"Creates a vector store from text chunks using local Hugging Face Embeddings.\"\"\"\n",
        "    try:\n",
        "        embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        )\n",
        "        vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
        "        return vector_store\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error creating vector store: {e}\")\n",
        "        raise\n",
        "\n",
        "def get_conversational_chain(vector_store):\n",
        "    \"\"\"Initializes the conversational chain with Gemini and memory.\"\"\"\n",
        "    try:\n",
        "        llm = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-2.5-flash\",\n",
        "            temperature=0.5,\n",
        "            # Explicitly avoid passing max_retries or other invalid params\n",
        "            google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
        "        )\n",
        "        memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "        conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=llm,\n",
        "            retriever=vector_store.as_retriever(),\n",
        "            memory=memory\n",
        "        )\n",
        "        return conversation_chain\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error initializing conversational chain: {e}\")\n",
        "        raise\n",
        "\n",
        "# Main Streamlit Application Logic\n",
        "def main():\n",
        "    st.set_page_config(page_title=\"DocuQuery: PDF AI Assistant\", page_icon=\"📄\")\n",
        "    st.title(\"📄 DocuQuery: AI-Powered PDF Knowledge Assistant\")\n",
        "    st.markdown(\"Upload your PDFs and ask questions about their content!\")\n",
        "\n",
        "    # Initialize Streamlit session state variables\n",
        "    if \"chat_history\" not in st.session_state:\n",
        "        st.session_state.chat_history = []\n",
        "    if \"conversation_chain\" not in st.session_state:\n",
        "        st.session_state.conversation_chain = None\n",
        "\n",
        "    with st.sidebar:\n",
        "        st.header(\"1. Upload Documents\")\n",
        "        pdf_docs = st.file_uploader(\n",
        "            \"Upload your PDF files here\",\n",
        "            accept_multiple_files=True,\n",
        "            type=\"pdf\"\n",
        "        )\n",
        "        if st.button(\"Process Documents\"):\n",
        "            with st.spinner(\"Processing PDFs...\"):\n",
        "                if pdf_docs:\n",
        "                    try:\n",
        "                        raw_text = get_pdf_text(pdf_docs)\n",
        "                        if not raw_text.strip():\n",
        "                            st.warning(\"No extractable text found in the PDFs.\")\n",
        "                        else:\n",
        "                            text_chunks = get_text_chunks(raw_text)\n",
        "                            vector_store = get_vector_store(text_chunks)\n",
        "                            st.session_state.conversation_chain = get_conversational_chain(vector_store)\n",
        "                            st.session_state.chat_history = [] # Clear chat history on new docs\n",
        "                            st.success(\"Documents processed! You can now ask questions.\")\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"Failed to process documents: {e}\")\n",
        "                else:\n",
        "                    st.warning(\"Please upload PDF files first.\")\n",
        "        st.markdown(\"---\")\n",
        "        st.caption(\"Powered by Vamsi(Data Science Student)\")\n",
        "\n",
        "    # Display chat messages from session state\n",
        "    for i, message in enumerate(st.session_state.chat_history):\n",
        "        if i % 2 == 0: # User message\n",
        "            with st.chat_message(\"user\"):\n",
        "                st.write(message)\n",
        "        else: # AI message\n",
        "            with st.chat_message(\"assistant\"):\n",
        "                st.write(message)\n",
        "\n",
        "    # User input chat box\n",
        "    user_question = st.chat_input(\"Ask a question about your documents...\")\n",
        "\n",
        "    if user_question:\n",
        "        if st.session_state.conversation_chain:\n",
        "            with st.spinner(\"Thinking...\"):\n",
        "                try:\n",
        "                    response = st.session_state.conversation_chain({\"question\": user_question})\n",
        "                    st.session_state.chat_history.append(user_question)\n",
        "                    st.session_state.chat_history.append(response[\"answer\"])\n",
        "                    st.rerun() # Rerun to update chat display\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error during conversation: {e}\")\n",
        "                    st.session_state.chat_history.append(user_question)\n",
        "                    st.session_state.chat_history.append(f\"Sorry, an error occurred: {e}\")\n",
        "                    st.rerun()\n",
        "        else:\n",
        "            st.warning(\"Please upload and process PDF documents first.\")\n",
        "            st.session_state.chat_history.append(user_question)\n",
        "            st.session_state.chat_history.append(\"Please upload and process PDF documents first.\")\n",
        "            st.rerun()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TYivnJn-_Sr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wpuydbz7_A4M"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok, conf\n",
        "from pyngrok.exception import PyngrokNgrokError\n",
        "\n",
        "ngrok.kill()\n",
        "# --- Configuration ---\n",
        "STREAMLIT_PORT = 8000 # The port your Streamlit app is running on\n",
        "\n",
        "# --- ngrok setup ---\n",
        "if NGROK_AUTH_TOKEN == \"YOUR_NGROK_AUTH_TOKEN\":\n",
        "    print(\"WARNING: Please replace 'YOUR_NGROK_AUTH_TOKEN' with your actual ngrok auth token.\")\n",
        "    print(\"You can get it from https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "else:\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# 1. Kill any existing ngrok processes\n",
        "print(\"Attempting to kill existing ngrok processes...\")\n",
        "\n",
        "\n",
        "# 2. Start Streamlit app in the background (if not already running)\n",
        "print(f\"Starting Streamlit app on port {STREAMLIT_PORT}...\")\n",
        "# This assumes your 'app.py' is in the current directory.\n",
        "# If your Streamlit app is already guaranteed to be running, you can comment this out.\n",
        "!nohup streamlit run app.py --server.port {STREAMLIT_PORT} &\n",
        "\n",
        "# 3. Create a new ngrok tunnel\n",
        "print(f\"Creating ngrok tunnel for port {STREAMLIT_PORT}...\")\n",
        "try:\n",
        "    public_url = ngrok.connect(STREAMLIT_PORT)\n",
        "    print(\"🚀 Streamlit App Public URL:\", public_url)\n",
        "    print(\"Remember to shut down the ngrok tunnel and Streamlit app when done.\")\n",
        "except PyngrokNgrokError as e:\n",
        "    print(f\"❌ Error creating ngrok tunnel: {e}\")\n",
        "    print(\"Ensure ngrok auth token is correct and Streamlit is running on the specified port.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ An unexpected error occurred: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
